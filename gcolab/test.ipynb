{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNbYjdJTNfWO9CZeEVHz45",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShadowofSkull/DevHack2023/blob/main/gcolab/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V-HSh8aXf8N",
        "outputId": "1b24b68e-b607-4847-84a3-88cdc74ffe7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2023-11-02 11:20:49.868091: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-02 11:20:49.868167: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-02 11:20:49.868208: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-02 11:20:51.159508: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2023-11-02 11:21:05.531190: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-02 11:21:05.531275: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-02 11:21:05.531321: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-02 11:21:07.407157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-md==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.6.0/en_core_web_md-3.6.0-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_md\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from heapq import nlargest"
      ],
      "metadata": {
        "id": "ISeWZZb3X09j"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cv =\"\"\"Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃƒÂ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education Details\n",
        "\n",
        "Data Science Assurance Associate\n",
        "\n",
        "Data Science Assurance Associate - Ernst & Young LLP\n",
        "Skill Details\n",
        "JAVASCRIPT- Exprience - 24 months\n",
        "jQuery- Exprience - 24 months\n",
        "Python- Exprience - 24 monthsCompany Details\n",
        "company - Ernst & Young LLP\n",
        "description - Fraud Investigations and Dispute Services   Assurance\n",
        "TECHNOLOGY ASSISTED REVIEW\n",
        "TAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.\n",
        "* Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review.\n",
        "* Understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring for the entire tool.\n",
        "* TAR assists in predictive coding, topic modelling from the evidence by following EY standards. Developed the classifier models in order to identify \"red flags\" and fraud-related issues.\n",
        "\n",
        "Tools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃƒÂ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment analysis. Matplot lib, Tableau dashboard for reporting.\n",
        "\n",
        "MULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)\n",
        "TEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA * Received customer feedback survey data for past one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4 categories.\n",
        "* Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words across all the Survey categories and plotted Word cloud.\n",
        "* Created customized tableau dashboards for effective reporting and visualizations.\n",
        "CHATBOT * Developed a user friendly chatbot for one of our Products which handle simple questions about hours of operation, reservation options and so on.\n",
        "* This chat bot serves entire product related questions. Giving overview of tool via QA platform and also give recommendation responses so that user question to build chain of relevant answer.\n",
        "* This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant /recommended questions.\n",
        "\n",
        "Tools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer\n",
        "\n",
        "INFORMATION GOVERNANCE\n",
        "Organizations to make informed decisions about all of the information they store. The integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk.\n",
        "* Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana.\n",
        "* Preforming ROT Analysis on the data which give information of data which helps identify content that is either Redundant, Outdated, or Trivial.\n",
        "* Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally identifiable information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks.\n",
        "Tools & Technologies: Python, Flask, Elastic Search, Kibana\n",
        "\n",
        "FRAUD ANALYTIC PLATFORM\n",
        "Fraud Analytics and investigative platform to review all red flag cases.\n",
        "Ã¢Â€Â¢ FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP systems.\n",
        "* It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics\n",
        "\n",
        "Tools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js\"\"\"\n",
        "\n",
        "cv2 = \"\"\"Education Details\n",
        "January 2016 B.Sc. Information Technology Mumbai, Maharashtra University of Mumbai\n",
        "January 2012 HSC  Allahabad, Uttar Pradesh Allahabad university\n",
        "January 2010 SSC dot Net Allahabad, Uttar Pradesh Allahabad university\n",
        "Web designer and Developer Trainer\n",
        "\n",
        "Web designer and Developer\n",
        "Skill Details\n",
        "Web design- Exprience - 12 months\n",
        "Php- Exprience - 12 monthsCompany Details\n",
        "company - NetTech India\n",
        "description - Working. ( salary - 12k)\n",
        "PERSONAL INTEREST\n",
        "\n",
        "Listening to Music, Surfing net, Watching Movie, Playing Cricket.\n",
        "company - EPI Center Academy\n",
        "description - Working.  ( Salary Contract based)\n",
        "company - Aptech Charni Road\n",
        "description - Salary Contract based)\"\"\"\n",
        "\n",
        "job = \"\"\"5 years+ experience in delivering stunning front-end productsSolid understanding of lean and agile practices, in particular SCRUMSoftware engineering practices: TDD, unit/functional automated testing, CI, CD, software design and architectureOutstanding software development talent proven by great work results/experience, hobby projects or open source contributionPassion for building great products and user-interfacesOpen minded, outgoing, self-confident and positive personalityCan do attitude, great team playerFluent in englishExcellent understanding of the whole web technology stack (Ruby/Ruby on Rails, JavaScript, SQL, HTML, CSS)Knowlege about cross-plattform responsive designSkill in writing scalable and multi platform frontend code.Â Knowledge about Frontend Performance OptimizationKnowledge about frontend libraries like jQuery, YUI, prototype and also JS coding without libraries.Basic knowledge of #URL_a58bd7bd48420a1f4774598bc5f1451bdcc79baee91a357c1d69e8aede501d73#, Python etc.\"\"\""
      ],
      "metadata": {
        "id": "ROyQeSwFYEDT"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_md')\n",
        "cv_doc = nlp(cv)\n",
        "print(len(list(cv_doc.sents)))\n",
        "job_doc = nlp(job)\n",
        "print(len(list(job_doc.sents)))\n",
        "for tok in cv_doc:\n",
        "  print(tok.text, tok.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkBeMWVVjq5j",
        "outputId": "3e1a637d-5d7c-456a-887e-bb9be8fb02cf"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44\n",
            "3\n",
            "Skills NOUN\n",
            "* PUNCT\n",
            "Programming NOUN\n",
            "Languages NOUN\n",
            ": PUNCT\n",
            "Python PROPN\n",
            "( PUNCT\n",
            "pandas NOUN\n",
            ", PUNCT\n",
            "numpy ADJ\n",
            ", PUNCT\n",
            "scipy NOUN\n",
            ", PUNCT\n",
            "scikit NOUN\n",
            "- PUNCT\n",
            "learn NOUN\n",
            ", PUNCT\n",
            "matplotlib PROPN\n",
            ") PUNCT\n",
            ", PUNCT\n",
            "Sql PROPN\n",
            ", PUNCT\n",
            "Java PROPN\n",
            ", PUNCT\n",
            "JavaScript PROPN\n",
            "/ SYM\n",
            "JQuery PROPN\n",
            ". PUNCT\n",
            "* PUNCT\n",
            "Machine NOUN\n",
            "learning NOUN\n",
            ": PUNCT\n",
            "Regression NOUN\n",
            ", PUNCT\n",
            "SVM PROPN\n",
            ", PUNCT\n",
            "NaÃƒÂ¯ve PROPN\n",
            "Bayes PROPN\n",
            ", PUNCT\n",
            "KNN PROPN\n",
            ", PUNCT\n",
            "Random PROPN\n",
            "Forest PROPN\n",
            ", PUNCT\n",
            "Decision PROPN\n",
            "Trees PROPN\n",
            ", PUNCT\n",
            "Boosting VERB\n",
            "techniques NOUN\n",
            ", PUNCT\n",
            "Cluster PROPN\n",
            "Analysis PROPN\n",
            ", PUNCT\n",
            "Word PROPN\n",
            "Embedding PROPN\n",
            ", PUNCT\n",
            "Sentiment PROPN\n",
            "Analysis PROPN\n",
            ", PUNCT\n",
            "Natural PROPN\n",
            "Language PROPN\n",
            "processing NOUN\n",
            ", PUNCT\n",
            "Dimensionality PROPN\n",
            "reduction NOUN\n",
            ", PUNCT\n",
            "Topic PROPN\n",
            "Modelling PROPN\n",
            "( PUNCT\n",
            "LDA PROPN\n",
            ", PUNCT\n",
            "NMF PROPN\n",
            ") PUNCT\n",
            ", PUNCT\n",
            "PCA PROPN\n",
            "& CCONJ\n",
            "Neural PROPN\n",
            "Nets PROPN\n",
            ". PUNCT\n",
            "* PUNCT\n",
            "Database PROPN\n",
            "Visualizations PROPN\n",
            ": PUNCT\n",
            "Mysql PROPN\n",
            ", PUNCT\n",
            "SqlServer PROPN\n",
            ", PUNCT\n",
            "Cassandra PROPN\n",
            ", PUNCT\n",
            "Hbase PROPN\n",
            ", PUNCT\n",
            "ElasticSearch PROPN\n",
            "D3.js NOUN\n",
            ", PUNCT\n",
            "DC.js NUM\n",
            ", PUNCT\n",
            "Plotly ADV\n",
            ", PUNCT\n",
            "kibana PROPN\n",
            ", PUNCT\n",
            "matplotlib PROPN\n",
            ", PUNCT\n",
            "ggplot NOUN\n",
            ", PUNCT\n",
            "Tableau PROPN\n",
            ". PUNCT\n",
            "* PUNCT\n",
            "Others NOUN\n",
            ": PUNCT\n",
            "Regular ADJ\n",
            "Expression PROPN\n",
            ", PUNCT\n",
            "HTML PROPN\n",
            ", PUNCT\n",
            "CSS PROPN\n",
            ", PUNCT\n",
            "Angular PROPN\n",
            "6 NUM\n",
            ", PUNCT\n",
            "Logstash PROPN\n",
            ", PUNCT\n",
            "Kafka PROPN\n",
            ", PUNCT\n",
            "Python PROPN\n",
            "Flask PROPN\n",
            ", PUNCT\n",
            "Git PROPN\n",
            ", PUNCT\n",
            "Docker PROPN\n",
            ", PUNCT\n",
            "computer NOUN\n",
            "vision NOUN\n",
            "- PUNCT\n",
            "Open ADJ\n",
            "CV NOUN\n",
            "and CCONJ\n",
            "understanding NOUN\n",
            "of ADP\n",
            "Deep PROPN\n",
            "learning NOUN\n",
            ". PUNCT\n",
            "Education NOUN\n",
            "Details PROPN\n",
            "\n",
            "\n",
            " SPACE\n",
            "Data PROPN\n",
            "Science PROPN\n",
            "Assurance PROPN\n",
            "Associate PROPN\n",
            "\n",
            "\n",
            " SPACE\n",
            "Data PROPN\n",
            "Science PROPN\n",
            "Assurance PROPN\n",
            "Associate PROPN\n",
            "- PUNCT\n",
            "Ernst PROPN\n",
            "& CCONJ\n",
            "Young PROPN\n",
            "LLP PROPN\n",
            "\n",
            " SPACE\n",
            "Skill PROPN\n",
            "Details NOUN\n",
            "\n",
            " SPACE\n",
            "JAVASCRIPT- PROPN\n",
            "Exprience PROPN\n",
            "- PUNCT\n",
            "24 NUM\n",
            "months NOUN\n",
            "\n",
            " SPACE\n",
            "jQuery- CCONJ\n",
            "Exprience PROPN\n",
            "- PUNCT\n",
            "24 NUM\n",
            "months NOUN\n",
            "\n",
            " SPACE\n",
            "Python- PROPN\n",
            "Exprience PROPN\n",
            "- PUNCT\n",
            "24 NUM\n",
            "monthsCompany NOUN\n",
            "Details NOUN\n",
            "\n",
            " SPACE\n",
            "company NOUN\n",
            "- PUNCT\n",
            "Ernst PROPN\n",
            "& CCONJ\n",
            "Young PROPN\n",
            "LLP PROPN\n",
            "\n",
            " SPACE\n",
            "description NOUN\n",
            "- PUNCT\n",
            "Fraud PROPN\n",
            "Investigations PROPN\n",
            "and CCONJ\n",
            "Dispute PROPN\n",
            "Services PROPN\n",
            "   SPACE\n",
            "Assurance PROPN\n",
            "\n",
            " SPACE\n",
            "TECHNOLOGY PROPN\n",
            "ASSISTED VERB\n",
            "REVIEW PROPN\n",
            "\n",
            " SPACE\n",
            "TAR PROPN\n",
            "( PUNCT\n",
            "Technology PROPN\n",
            "Assisted PROPN\n",
            "Review PROPN\n",
            ") PUNCT\n",
            "assists VERB\n",
            "in ADP\n",
            "accelerating VERB\n",
            "the DET\n",
            "review NOUN\n",
            "process NOUN\n",
            "and CCONJ\n",
            "run VERB\n",
            "analytics NOUN\n",
            "and CCONJ\n",
            "generate VERB\n",
            "reports NOUN\n",
            ". PUNCT\n",
            "\n",
            " SPACE\n",
            "* PUNCT\n",
            "Core NOUN\n",
            "member NOUN\n",
            "of ADP\n",
            "a DET\n",
            "team NOUN\n",
            "helped VERB\n",
            "in ADP\n",
            "developing VERB\n",
            "automated VERB\n",
            "review NOUN\n",
            "platform NOUN\n",
            "tool NOUN\n",
            "from ADP\n",
            "scratch NOUN\n",
            "for ADP\n",
            "assisting VERB\n",
            "E NOUN\n",
            "discovery NOUN\n",
            "domain NOUN\n",
            ", PUNCT\n",
            "this DET\n",
            "tool NOUN\n",
            "implements VERB\n",
            "predictive ADJ\n",
            "coding NOUN\n",
            "and CCONJ\n",
            "topic NOUN\n",
            "modelling NOUN\n",
            "by ADP\n",
            "automating VERB\n",
            "reviews NOUN\n",
            ", PUNCT\n",
            "resulting VERB\n",
            "in ADP\n",
            "reduced VERB\n",
            "labor NOUN\n",
            "costs NOUN\n",
            "and CCONJ\n",
            "time NOUN\n",
            "spent VERB\n",
            "during ADP\n",
            "the DET\n",
            "lawyers NOUN\n",
            "review VERB\n",
            ". PUNCT\n",
            "\n",
            " SPACE\n",
            "* PUNCT\n",
            "Understand VERB\n",
            "the DET\n",
            "end NOUN\n",
            "to ADP\n",
            "end NOUN\n",
            "flow NOUN\n",
            "of ADP\n",
            "the DET\n",
            "solution NOUN\n",
            ", PUNCT\n",
            "doing VERB\n",
            "research NOUN\n",
            "and CCONJ\n",
            "development NOUN\n",
            "for ADP\n",
            "classification NOUN\n",
            "models NOUN\n",
            ", PUNCT\n",
            "predictive ADJ\n",
            "analysis NOUN\n",
            "and CCONJ\n",
            "mining NOUN\n",
            "of ADP\n",
            "the DET\n",
            "information NOUN\n",
            "present ADJ\n",
            "in ADP\n",
            "text NOUN\n",
            "data NOUN\n",
            ". PUNCT\n",
            "Worked VERB\n",
            "on ADP\n",
            "analyzing VERB\n",
            "the DET\n",
            "outputs NOUN\n",
            "and CCONJ\n",
            "precision NOUN\n",
            "monitoring NOUN\n",
            "for ADP\n",
            "the DET\n",
            "entire ADJ\n",
            "tool NOUN\n",
            ". PUNCT\n",
            "\n",
            " SPACE\n",
            "* PUNCT\n",
            "TAR NOUN\n",
            "assists NOUN\n",
            "in ADP\n",
            "predictive ADJ\n",
            "coding NOUN\n",
            ", PUNCT\n",
            "topic NOUN\n",
            "modelling NOUN\n",
            "from ADP\n",
            "the DET\n",
            "evidence NOUN\n",
            "by ADP\n",
            "following VERB\n",
            "EY PROPN\n",
            "standards NOUN\n",
            ". PUNCT\n",
            "Developed VERB\n",
            "the DET\n",
            "classifier ADJ\n",
            "models NOUN\n",
            "in ADP\n",
            "order NOUN\n",
            "to PART\n",
            "identify VERB\n",
            "\" PUNCT\n",
            "red ADJ\n",
            "flags NOUN\n",
            "\" PUNCT\n",
            "and CCONJ\n",
            "fraud NOUN\n",
            "- PUNCT\n",
            "related VERB\n",
            "issues NOUN\n",
            ". PUNCT\n",
            "\n",
            "\n",
            " SPACE\n",
            "Tools PROPN\n",
            "& CCONJ\n",
            "Technologies PROPN\n",
            ": PUNCT\n",
            "Python PROPN\n",
            ", PUNCT\n",
            "scikit NOUN\n",
            "- PUNCT\n",
            "learn NOUN\n",
            ", PUNCT\n",
            "tfidf PROPN\n",
            ", PUNCT\n",
            "word2vec PROPN\n",
            ", PUNCT\n",
            "doc2vec PROPN\n",
            ", PUNCT\n",
            "cosine NOUN\n",
            "similarity NOUN\n",
            ", PUNCT\n",
            "NaÃƒÂ¯ve PROPN\n",
            "Bayes PROPN\n",
            ", PUNCT\n",
            "LDA PROPN\n",
            ", PUNCT\n",
            "NMF PROPN\n",
            "for ADP\n",
            "topic NOUN\n",
            "modelling NOUN\n",
            ", PUNCT\n",
            "Vader PROPN\n",
            "and CCONJ\n",
            "text NOUN\n",
            "blob NOUN\n",
            "for ADP\n",
            "sentiment NOUN\n",
            "analysis NOUN\n",
            ". PUNCT\n",
            "Matplot NOUN\n",
            "lib NOUN\n",
            ", PUNCT\n",
            "Tableau PROPN\n",
            "dashboard NOUN\n",
            "for ADP\n",
            "reporting VERB\n",
            ". PUNCT\n",
            "\n",
            "\n",
            " SPACE\n",
            "MULTIPLE PROPN\n",
            "DATA PROPN\n",
            "SCIENCE NOUN\n",
            "AND CCONJ\n",
            "ANALYTIC ADJ\n",
            "PROJECTS NOUN\n",
            "( PUNCT\n",
            "USA PROPN\n",
            "CLIENTS NOUN\n",
            ") PUNCT\n",
            "\n",
            " SPACE\n",
            "TEXT PROPN\n",
            "ANALYTICS NOUN\n",
            "- PUNCT\n",
            "MOTOR PROPN\n",
            "VEHICLE PROPN\n",
            "CUSTOMER PROPN\n",
            "REVIEW PROPN\n",
            "DATA PROPN\n",
            "* PUNCT\n",
            "Received VERB\n",
            "customer NOUN\n",
            "feedback NOUN\n",
            "survey NOUN\n",
            "data NOUN\n",
            "for ADP\n",
            "past ADJ\n",
            "one NUM\n",
            "year NOUN\n",
            ". PUNCT\n",
            "Performed VERB\n",
            "sentiment NOUN\n",
            "( PUNCT\n",
            "Positive PROPN\n",
            ", PUNCT\n",
            "Negative PROPN\n",
            "& CCONJ\n",
            "Neutral PROPN\n",
            ") PUNCT\n",
            "and CCONJ\n",
            "time NOUN\n",
            "series NOUN\n",
            "analysis NOUN\n",
            "on ADP\n",
            "customer NOUN\n",
            "comments NOUN\n",
            "across ADP\n",
            "all DET\n",
            "4 NUM\n",
            "categories NOUN\n",
            ". PUNCT\n",
            "\n",
            " SPACE\n",
            "* PUNCT\n",
            "Created VERB\n",
            "heat NOUN\n",
            "map NOUN\n",
            "of ADP\n",
            "terms NOUN\n",
            "by ADP\n",
            "survey NOUN\n",
            "category NOUN\n",
            "based VERB\n",
            "on ADP\n",
            "frequency NOUN\n",
            "of ADP\n",
            "words NOUN\n",
            "* PUNCT\n",
            "Extracted VERB\n",
            "Positive PROPN\n",
            "and CCONJ\n",
            "Negative ADJ\n",
            "words NOUN\n",
            "across ADP\n",
            "all DET\n",
            "the DET\n",
            "Survey PROPN\n",
            "categories NOUN\n",
            "and CCONJ\n",
            "plotted VERB\n",
            "Word PROPN\n",
            "cloud NOUN\n",
            ". PUNCT\n",
            "\n",
            " SPACE\n",
            "* PUNCT\n",
            "Created VERB\n",
            "customized VERB\n",
            "tableau NOUN\n",
            "dashboards NOUN\n",
            "for ADP\n",
            "effective ADJ\n",
            "reporting NOUN\n",
            "and CCONJ\n",
            "visualizations NOUN\n",
            ". PUNCT\n",
            "\n",
            " SPACE\n",
            "CHATBOT NOUN\n",
            "* PUNCT\n",
            "Developed VERB\n",
            "a DET\n",
            "user NOUN\n",
            "friendly ADJ\n",
            "chatbot NOUN\n",
            "for ADP\n",
            "one NUM\n",
            "of ADP\n",
            "our PRON\n",
            "Products NOUN\n",
            "which PRON\n",
            "handle VERB\n",
            "simple ADJ\n",
            "questions NOUN\n",
            "about ADP\n",
            "hours NOUN\n",
            "of ADP\n",
            "operation NOUN\n",
            ", PUNCT\n",
            "reservation NOUN\n",
            "options NOUN\n",
            "and CCONJ\n",
            "so ADV\n",
            "on ADV\n",
            ". PUNCT\n",
            "\n",
            " SPACE\n",
            "* PUNCT\n",
            "This DET\n",
            "chat NOUN\n",
            "bot NOUN\n",
            "serves VERB\n",
            "entire ADJ\n",
            "product NOUN\n",
            "related VERB\n",
            "questions NOUN\n",
            ". PUNCT\n",
            "Giving VERB\n",
            "overview NOUN\n",
            "of ADP\n",
            "tool NOUN\n",
            "via ADP\n",
            "QA PROPN\n",
            "platform NOUN\n",
            "and CCONJ\n",
            "also ADV\n",
            "give VERB\n",
            "recommendation NOUN\n",
            "responses NOUN\n",
            "so SCONJ\n",
            "that SCONJ\n",
            "user NOUN\n",
            "question NOUN\n",
            "to PART\n",
            "build VERB\n",
            "chain NOUN\n",
            "of ADP\n",
            "relevant ADJ\n",
            "answer NOUN\n",
            ". PUNCT\n",
            "\n",
            " SPACE\n",
            "* PUNCT\n",
            "This PRON\n",
            "too ADV\n",
            "has VERB\n",
            "intelligence NOUN\n",
            "to PART\n",
            "build VERB\n",
            "the DET\n",
            "pipeline NOUN\n",
            "of ADP\n",
            "questions NOUN\n",
            "as ADP\n",
            "per ADP\n",
            "user NOUN\n",
            "requirement NOUN\n",
            "and CCONJ\n",
            "asks VERB\n",
            "the DET\n",
            "relevant ADJ\n",
            "/recommended VERB\n",
            "questions NOUN\n",
            ". PUNCT\n",
            "\n",
            "\n",
            " SPACE\n",
            "Tools PROPN\n",
            "& CCONJ\n",
            "Technologies PROPN\n",
            ": PUNCT\n",
            "Python PROPN\n",
            ", PUNCT\n",
            "Natural ADJ\n",
            "language NOUN\n",
            "processing NOUN\n",
            ", PUNCT\n",
            "NLTK PROPN\n",
            ", PUNCT\n",
            "spacy NOUN\n",
            ", PUNCT\n",
            "topic NOUN\n",
            "modelling NOUN\n",
            ", PUNCT\n",
            "Sentiment NOUN\n",
            "analysis NOUN\n",
            ", PUNCT\n",
            "Word PROPN\n",
            "Embedding PROPN\n",
            ", PUNCT\n",
            "scikit NOUN\n",
            "- PUNCT\n",
            "learn NOUN\n",
            ", PUNCT\n",
            "JavaScript PROPN\n",
            "/ SYM\n",
            "JQuery PROPN\n",
            ", PUNCT\n",
            "SqlServer PROPN\n",
            "\n",
            "\n",
            " SPACE\n",
            "INFORMATION NOUN\n",
            "GOVERNANCE PROPN\n",
            "\n",
            " SPACE\n",
            "Organizations PROPN\n",
            "to PART\n",
            "make VERB\n",
            "informed ADJ\n",
            "decisions NOUN\n",
            "about ADP\n",
            "all PRON\n",
            "of ADP\n",
            "the DET\n",
            "information NOUN\n",
            "they PRON\n",
            "store VERB\n",
            ". PUNCT\n",
            "The DET\n",
            "integrated ADJ\n",
            "Information PROPN\n",
            "Governance PROPN\n",
            "portfolio NOUN\n",
            "synthesizes VERB\n",
            "intelligence NOUN\n",
            "across ADP\n",
            "unstructured ADJ\n",
            "data NOUN\n",
            "sources NOUN\n",
            "and CCONJ\n",
            "facilitates VERB\n",
            "action NOUN\n",
            "to PART\n",
            "ensure VERB\n",
            "organizations NOUN\n",
            "are AUX\n",
            "best ADV\n",
            "positioned VERB\n",
            "to PART\n",
            "counter VERB\n",
            "information NOUN\n",
            "risk NOUN\n",
            ". PUNCT\n",
            "\n",
            " SPACE\n",
            "* PUNCT\n",
            "Scan PROPN\n",
            "data NOUN\n",
            "from ADP\n",
            "multiple ADJ\n",
            "sources NOUN\n",
            "of ADP\n",
            "formats NOUN\n",
            "and CCONJ\n",
            "parse VERB\n",
            "different ADJ\n",
            "file NOUN\n",
            "formats NOUN\n",
            ", PUNCT\n",
            "extract VERB\n",
            "Meta NOUN\n",
            "data NOUN\n",
            "information NOUN\n",
            ", PUNCT\n",
            "push VERB\n",
            "results NOUN\n",
            "for ADP\n",
            "indexing VERB\n",
            "elastic ADJ\n",
            "search NOUN\n",
            "and CCONJ\n",
            "created VERB\n",
            "customized VERB\n",
            ", PUNCT\n",
            "interactive ADJ\n",
            "dashboards NOUN\n",
            "using VERB\n",
            "kibana PROPN\n",
            ". PUNCT\n",
            "\n",
            " SPACE\n",
            "* PUNCT\n",
            "Preforming VERB\n",
            "ROT PROPN\n",
            "Analysis PROPN\n",
            "on ADP\n",
            "the DET\n",
            "data NOUN\n",
            "which PRON\n",
            "give VERB\n",
            "information NOUN\n",
            "of ADP\n",
            "data NOUN\n",
            "which PRON\n",
            "helps AUX\n",
            "identify VERB\n",
            "content NOUN\n",
            "that PRON\n",
            "is AUX\n",
            "either CCONJ\n",
            "Redundant ADJ\n",
            ", PUNCT\n",
            "Outdated PROPN\n",
            ", PUNCT\n",
            "or CCONJ\n",
            "Trivial PROPN\n",
            ". PUNCT\n",
            "\n",
            " SPACE\n",
            "* PUNCT\n",
            "Preforming VERB\n",
            "full ADJ\n",
            "- PUNCT\n",
            "text NOUN\n",
            "search NOUN\n",
            "analysis NOUN\n",
            "on ADP\n",
            "elastic ADJ\n",
            "search NOUN\n",
            "with ADP\n",
            "predefined VERB\n",
            "methods NOUN\n",
            "which PRON\n",
            "can AUX\n",
            "tag VERB\n",
            "as ADP\n",
            "( PUNCT\n",
            "PII PROPN\n",
            ") PUNCT\n",
            "personally ADV\n",
            "identifiable ADJ\n",
            "information NOUN\n",
            "( PUNCT\n",
            "social ADJ\n",
            "security NOUN\n",
            "numbers NOUN\n",
            ", PUNCT\n",
            "addresses NOUN\n",
            ", PUNCT\n",
            "names NOUN\n",
            ", PUNCT\n",
            "etc X\n",
            ". X\n",
            ") PUNCT\n",
            "which PRON\n",
            "frequently ADV\n",
            "targeted VERB\n",
            "during ADP\n",
            "cyber NOUN\n",
            "- PUNCT\n",
            "attacks NOUN\n",
            ". PUNCT\n",
            "\n",
            " SPACE\n",
            "Tools PROPN\n",
            "& CCONJ\n",
            "Technologies PROPN\n",
            ": PUNCT\n",
            "Python PROPN\n",
            ", PUNCT\n",
            "Flask PROPN\n",
            ", PUNCT\n",
            "Elastic PROPN\n",
            "Search PROPN\n",
            ", PUNCT\n",
            "Kibana PROPN\n",
            "\n",
            "\n",
            " SPACE\n",
            "FRAUD VERB\n",
            "ANALYTIC PROPN\n",
            "PLATFORM PROPN\n",
            "\n",
            " SPACE\n",
            "Fraud PROPN\n",
            "Analytics PROPN\n",
            "and CCONJ\n",
            "investigative ADJ\n",
            "platform NOUN\n",
            "to PART\n",
            "review VERB\n",
            "all DET\n",
            "red ADJ\n",
            "flag NOUN\n",
            "cases NOUN\n",
            ". PUNCT\n",
            "\n",
            " SPACE\n",
            "Ã¢Â€Â¢ PROPN\n",
            "FAP PROPN\n",
            "is AUX\n",
            "a DET\n",
            "Fraud PROPN\n",
            "Analytics PROPN\n",
            "and CCONJ\n",
            "investigative ADJ\n",
            "platform NOUN\n",
            "with ADP\n",
            "inbuilt VERB\n",
            "case NOUN\n",
            "manager NOUN\n",
            "and CCONJ\n",
            "suite NOUN\n",
            "of ADP\n",
            "Analytics NOUN\n",
            "for ADP\n",
            "various ADJ\n",
            "ERP PROPN\n",
            "systems NOUN\n",
            ". PUNCT\n",
            "\n",
            " SPACE\n",
            "* PUNCT\n",
            "It PRON\n",
            "can AUX\n",
            "be AUX\n",
            "used VERB\n",
            "by ADP\n",
            "clients NOUN\n",
            "to PART\n",
            "interrogate VERB\n",
            "their PRON\n",
            "Accounting PROPN\n",
            "systems NOUN\n",
            "for ADP\n",
            "identifying VERB\n",
            "the DET\n",
            "anomalies NOUN\n",
            "which PRON\n",
            "can AUX\n",
            "be AUX\n",
            "indicators NOUN\n",
            "of ADP\n",
            "fraud NOUN\n",
            "by ADP\n",
            "running VERB\n",
            "advanced ADJ\n",
            "analytics NOUN\n",
            "\n",
            "\n",
            " SPACE\n",
            "Tools PROPN\n",
            "& CCONJ\n",
            "Technologies PROPN\n",
            ": PUNCT\n",
            "HTML PROPN\n",
            ", PUNCT\n",
            "JavaScript PROPN\n",
            ", PUNCT\n",
            "SqlServer PROPN\n",
            ", PUNCT\n",
            "JQuery PROPN\n",
            ", PUNCT\n",
            "CSS PROPN\n",
            ", PUNCT\n",
            "Bootstrap PROPN\n",
            ", PUNCT\n",
            "Node.js PROPN\n",
            ", PUNCT\n",
            "D3.js PROPN\n",
            ", PUNCT\n",
            "DC.js NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher\n",
        "# To get nouns\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [\n",
        "    {\"POS\": \"PROPN\", \"OP\": \"+\"},\n",
        "    {\"POS\": \"NOUN\", \"OP\": \"+\"},\n",
        "    {\"IS_PUNCT\": True, \"OP\": \"!\"}\n",
        "\n",
        "]\n",
        "pattern1 = [\n",
        "    {\"POS\": \"NOUN\", \"OP\": \"+\"},\n",
        "    {\"POS\": \"PROPN\", \"OP\": \"+\"},\n",
        "    {\"IS_PUNCT\": True, \"OP\": \"!\"}\n",
        "\n",
        "]\n",
        "pattern2 = [\n",
        "    {\"POS\": \"NOUN\", \"OP\": \"+\"},\n",
        "    {\"IS_PUNCT\": True, \"OP\": \"!\"}\n",
        "    {\"POS\": True, \"OP\": \"!\"}\n",
        "\n",
        "]\n",
        "pattern3 = [\n",
        "    {\"POS\": \"PROPN\", \"OP\": \"+\"},\n",
        "    {\"IS_PUNCT\": True, \"OP\": \"!\"}\n",
        "\n",
        "]\n",
        "matcher.add(\"label\", [pattern, pattern1, pattern2, pattern3], greedy=\"LONGEST\")\n",
        "matches = matcher(cv_doc)\n",
        "# To sort in the order of occurance in the resume\n",
        "matches.sort(key = lambda x: x[1])\n",
        "print(len(matches))\n",
        "keyword = []\n",
        "stopwords = list(STOP_WORDS)\n",
        "\n",
        "\n",
        "for mat in matches:\n",
        "  text = cv_doc[mat[1]: mat[2]]\n",
        "  if not (text.text in stopwords or text.text in punctuation):\n",
        "    keyword.append(text.text)\n",
        "    print(mat, text.text)\n",
        "\n",
        "print(keyword)\n",
        "\n",
        "# pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
        "# pos_tag = ['PROPN', 'NOUN']\n",
        "\n",
        "# for token in cv_doc:\n",
        "#     if(token.text in stopwords or token.text in punctuation):\n",
        "#         continue\n",
        "    # if(token.pos_ in pos_tag):\n",
        "    #     keyword.append(token.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEGEgYGBkCrh",
        "outputId": "89e034a5-6923-4c9d-ad13-09b4bb3f115e"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "162\n",
            "(7407097451842649916, 2, 4) Programming Languages\n",
            "(7407097451842649916, 29, 31) Machine learning\n",
            "(7407097451842649916, 36, 38) NaÃƒÂ¯ve Bayes\n",
            "(7407097451842649916, 41, 43) Random Forest\n",
            "(7407097451842649916, 44, 46) Decision Trees\n",
            "(7407097451842649916, 50, 52) Cluster Analysis\n",
            "(7407097451842649916, 53, 55) Word Embedding\n",
            "(7407097451842649916, 56, 58) Sentiment Analysis\n",
            "(7407097451842649916, 59, 62) Natural Language processing\n",
            "(7407097451842649916, 63, 65) Dimensionality reduction\n",
            "(7407097451842649916, 66, 68) Topic Modelling\n",
            "(7407097451842649916, 76, 78) Neural Nets\n",
            "(7407097451842649916, 80, 82) Database Visualizations\n",
            "(7407097451842649916, 91, 93) ElasticSearch D3.js\n",
            "(7407097451842649916, 116, 118) Angular 6\n",
            "(7407097451842649916, 123, 125) Python Flask\n",
            "(7407097451842649916, 130, 132) computer vision\n",
            "(7407097451842649916, 134, 136) CV and\n",
            "(7407097451842649916, 136, 138) understanding of\n",
            "(7407097451842649916, 138, 140) Deep learning\n",
            "(7407097451842649916, 141, 144) Education Details \n",
            "\n",
            "\n",
            "(7407097451842649916, 144, 149) Data Science Assurance Associate \n",
            "\n",
            "\n",
            "(7407097451842649916, 149, 153) Data Science Assurance Associate\n",
            "(7407097451842649916, 156, 159) Young LLP\n",
            "\n",
            "(7407097451842649916, 159, 162) Skill Details \n",
            "\n",
            "(7407097451842649916, 162, 164) JAVASCRIPT- Exprience\n",
            "(7407097451842649916, 166, 168) months\n",
            "\n",
            "(7407097451842649916, 172, 174) months\n",
            "\n",
            "(7407097451842649916, 174, 176) Python- Exprience\n",
            "(7407097451842649916, 178, 181) monthsCompany Details \n",
            "\n",
            "(7407097451842649916, 185, 188) Young LLP\n",
            "\n",
            "(7407097451842649916, 190, 193) Fraud Investigations and\n",
            "(7407097451842649916, 193, 196) Dispute Services   \n",
            "(7407097451842649916, 196, 198) Assurance\n",
            "\n",
            "(7407097451842649916, 198, 200) TECHNOLOGY ASSISTED\n",
            "(7407097451842649916, 200, 202) REVIEW\n",
            "\n",
            "(7407097451842649916, 204, 207) Technology Assisted Review\n",
            "(7407097451842649916, 212, 215) review process and\n",
            "(7407097451842649916, 216, 218) analytics and\n",
            "(7407097451842649916, 223, 226) Core member of\n",
            "(7407097451842649916, 227, 229) team helped\n",
            "(7407097451842649916, 232, 236) review platform tool from\n",
            "(7407097451842649916, 236, 238) scratch for\n",
            "(7407097451842649916, 239, 242) E discovery domain\n",
            "(7407097451842649916, 244, 246) tool implements\n",
            "(7407097451842649916, 247, 249) coding and\n",
            "(7407097451842649916, 249, 252) topic modelling by\n",
            "(7407097451842649916, 258, 261) labor costs and\n",
            "(7407097451842649916, 261, 263) time spent\n",
            "(7407097451842649916, 265, 267) lawyers review\n",
            "(7407097451842649916, 272, 274) end to\n",
            "(7407097451842649916, 274, 277) end flow of\n",
            "(7407097451842649916, 281, 283) research and\n",
            "(7407097451842649916, 283, 285) development for\n",
            "(7407097451842649916, 285, 287) classification models\n",
            "(7407097451842649916, 289, 291) analysis and\n",
            "(7407097451842649916, 291, 293) mining of\n",
            "(7407097451842649916, 294, 296) information present\n",
            "(7407097451842649916, 297, 299) text data\n",
            "(7407097451842649916, 304, 306) outputs and\n",
            "(7407097451842649916, 306, 309) precision monitoring for\n",
            "(7407097451842649916, 315, 318) TAR assists in\n",
            "(7407097451842649916, 321, 324) topic modelling from\n",
            "(7407097451842649916, 325, 327) evidence by\n",
            "(7407097451842649916, 328, 330) EY standards\n",
            "(7407097451842649916, 334, 336) models in\n",
            "(7407097451842649916, 336, 338) order to\n",
            "(7407097451842649916, 366, 368) cosine similarity\n",
            "(7407097451842649916, 369, 371) NaÃƒÂ¯ve Bayes\n",
            "(7407097451842649916, 374, 376) NMF for\n",
            "(7407097451842649916, 376, 378) topic modelling\n",
            "(7407097451842649916, 379, 381) Vader and\n",
            "(7407097451842649916, 381, 384) text blob for\n",
            "(7407097451842649916, 384, 386) sentiment analysis\n",
            "(7407097451842649916, 387, 389) Matplot lib\n",
            "(7407097451842649916, 390, 393) Tableau dashboard for\n",
            "(7407097451842649916, 396, 400) MULTIPLE DATA SCIENCE AND\n",
            "(7407097451842649916, 403, 405) USA CLIENTS\n",
            "(7407097451842649916, 407, 409) TEXT ANALYTICS\n",
            "(7407097451842649916, 410, 415) MOTOR VEHICLE CUSTOMER REVIEW DATA\n",
            "(7407097451842649916, 417, 422) customer feedback survey data for\n",
            "(7407097451842649916, 436, 440) time series analysis on\n",
            "(7407097451842649916, 440, 443) customer comments across\n",
            "(7407097451842649916, 450, 453) heat map of\n",
            "(7407097451842649916, 453, 455) terms by\n",
            "(7407097451842649916, 455, 458) survey category based\n",
            "(7407097451842649916, 459, 461) frequency of\n",
            "(7407097451842649916, 464, 466) Positive and\n",
            "(7407097451842649916, 467, 469) words across\n",
            "(7407097451842649916, 471, 474) Survey categories and\n",
            "(7407097451842649916, 475, 477) Word cloud\n",
            "(7407097451842649916, 482, 485) tableau dashboards for\n",
            "(7407097451842649916, 486, 488) reporting and\n",
            "(7407097451842649916, 495, 497) user friendly\n",
            "(7407097451842649916, 497, 499) chatbot for\n",
            "(7407097451842649916, 502, 504) Products which\n",
            "(7407097451842649916, 506, 508) questions about\n",
            "(7407097451842649916, 508, 510) hours of\n",
            "(7407097451842649916, 512, 515) reservation options and\n",
            "(7407097451842649916, 521, 524) chat bot serves\n",
            "(7407097451842649916, 525, 527) product related\n",
            "(7407097451842649916, 530, 532) overview of\n",
            "(7407097451842649916, 532, 534) tool via\n",
            "(7407097451842649916, 534, 537) QA platform and\n",
            "(7407097451842649916, 539, 542) recommendation responses so\n",
            "(7407097451842649916, 543, 546) user question to\n",
            "(7407097451842649916, 547, 549) chain of\n",
            "(7407097451842649916, 557, 559) intelligence to\n",
            "(7407097451842649916, 561, 563) pipeline of\n",
            "(7407097451842649916, 563, 565) questions as\n",
            "(7407097451842649916, 566, 569) user requirement and\n",
            "(7407097451842649916, 583, 585) language processing\n",
            "(7407097451842649916, 590, 592) topic modelling\n",
            "(7407097451842649916, 593, 595) Sentiment analysis\n",
            "(7407097451842649916, 596, 598) Word Embedding\n",
            "(7407097451842649916, 607, 609) SqlServer\n",
            "\n",
            "\n",
            "(7407097451842649916, 609, 612) INFORMATION GOVERNANCE\n",
            "\n",
            "(7407097451842649916, 612, 614) Organizations to\n",
            "(7407097451842649916, 616, 618) decisions about\n",
            "(7407097451842649916, 621, 623) information they\n",
            "(7407097451842649916, 627, 631) Information Governance portfolio synthesizes\n",
            "(7407097451842649916, 631, 633) intelligence across\n",
            "(7407097451842649916, 634, 637) data sources and\n",
            "(7407097451842649916, 638, 640) action to\n",
            "(7407097451842649916, 641, 643) organizations are\n",
            "(7407097451842649916, 647, 649) information risk\n",
            "(7407097451842649916, 652, 655) Scan data from\n",
            "(7407097451842649916, 656, 658) sources of\n",
            "(7407097451842649916, 658, 660) formats and\n",
            "(7407097451842649916, 662, 664) file formats\n",
            "(7407097451842649916, 666, 669) Meta data information\n",
            "(7407097451842649916, 671, 673) results for\n",
            "(7407097451842649916, 675, 677) search and\n",
            "(7407097451842649916, 681, 683) dashboards using\n",
            "(7407097451842649916, 688, 691) ROT Analysis on\n",
            "(7407097451842649916, 692, 694) data which\n",
            "(7407097451842649916, 695, 697) information of\n",
            "(7407097451842649916, 697, 699) data which\n",
            "(7407097451842649916, 701, 703) content that\n",
            "(7407097451842649916, 717, 721) text search analysis on\n",
            "(7407097451842649916, 722, 724) search with\n",
            "(7407097451842649916, 725, 727) methods which\n",
            "(7407097451842649916, 738, 740) security numbers\n",
            "(7407097451842649916, 765, 767) Elastic Search\n",
            "(7407097451842649916, 768, 770) Kibana\n",
            "\n",
            "\n",
            "(7407097451842649916, 771, 774) ANALYTIC PLATFORM\n",
            "\n",
            "(7407097451842649916, 774, 777) Fraud Analytics and\n",
            "(7407097451842649916, 778, 780) platform to\n",
            "(7407097451842649916, 783, 785) flag cases\n",
            "(7407097451842649916, 787, 790) Ã¢Â€Â¢ FAP is\n",
            "(7407097451842649916, 791, 794) Fraud Analytics and\n",
            "(7407097451842649916, 795, 797) platform with\n",
            "(7407097451842649916, 798, 801) case manager and\n",
            "(7407097451842649916, 801, 803) suite of\n",
            "(7407097451842649916, 803, 805) Analytics for\n",
            "(7407097451842649916, 806, 808) ERP systems\n",
            "(7407097451842649916, 816, 818) clients to\n",
            "(7407097451842649916, 820, 823) Accounting systems for\n",
            "(7407097451842649916, 825, 827) anomalies which\n",
            "(7407097451842649916, 829, 831) indicators of\n",
            "(7407097451842649916, 831, 833) fraud by\n",
            "(7407097451842649916, 835, 837) analytics\n",
            "\n",
            "\n",
            "['Programming Languages', 'Machine learning', 'NaÃƒÂ¯ve Bayes', 'Random Forest', 'Decision Trees', 'Cluster Analysis', 'Word Embedding', 'Sentiment Analysis', 'Natural Language processing', 'Dimensionality reduction', 'Topic Modelling', 'Neural Nets', 'Database Visualizations', 'ElasticSearch D3.js', 'Angular 6', 'Python Flask', 'computer vision', 'CV and', 'understanding of', 'Deep learning', 'Education Details \\n\\n', 'Data Science Assurance Associate \\n\\n', 'Data Science Assurance Associate', 'Young LLP\\n', 'Skill Details \\n', 'JAVASCRIPT- Exprience', 'months\\n', 'months\\n', 'Python- Exprience', 'monthsCompany Details \\n', 'Young LLP\\n', 'Fraud Investigations and', 'Dispute Services   ', 'Assurance\\n', 'TECHNOLOGY ASSISTED', 'REVIEW\\n', 'Technology Assisted Review', 'review process and', 'analytics and', 'Core member of', 'team helped', 'review platform tool from', 'scratch for', 'E discovery domain', 'tool implements', 'coding and', 'topic modelling by', 'labor costs and', 'time spent', 'lawyers review', 'end to', 'end flow of', 'research and', 'development for', 'classification models', 'analysis and', 'mining of', 'information present', 'text data', 'outputs and', 'precision monitoring for', 'TAR assists in', 'topic modelling from', 'evidence by', 'EY standards', 'models in', 'order to', 'cosine similarity', 'NaÃƒÂ¯ve Bayes', 'NMF for', 'topic modelling', 'Vader and', 'text blob for', 'sentiment analysis', 'Matplot lib', 'Tableau dashboard for', 'MULTIPLE DATA SCIENCE AND', 'USA CLIENTS', 'TEXT ANALYTICS', 'MOTOR VEHICLE CUSTOMER REVIEW DATA', 'customer feedback survey data for', 'time series analysis on', 'customer comments across', 'heat map of', 'terms by', 'survey category based', 'frequency of', 'Positive and', 'words across', 'Survey categories and', 'Word cloud', 'tableau dashboards for', 'reporting and', 'user friendly', 'chatbot for', 'Products which', 'questions about', 'hours of', 'reservation options and', 'chat bot serves', 'product related', 'overview of', 'tool via', 'QA platform and', 'recommendation responses so', 'user question to', 'chain of', 'intelligence to', 'pipeline of', 'questions as', 'user requirement and', 'language processing', 'topic modelling', 'Sentiment analysis', 'Word Embedding', 'SqlServer\\n\\n', 'INFORMATION GOVERNANCE\\n', 'Organizations to', 'decisions about', 'information they', 'Information Governance portfolio synthesizes', 'intelligence across', 'data sources and', 'action to', 'organizations are', 'information risk', 'Scan data from', 'sources of', 'formats and', 'file formats', 'Meta data information', 'results for', 'search and', 'dashboards using', 'ROT Analysis on', 'data which', 'information of', 'data which', 'content that', 'text search analysis on', 'search with', 'methods which', 'security numbers', 'Elastic Search', 'Kibana\\n\\n', 'ANALYTIC PLATFORM\\n', 'Fraud Analytics and', 'platform to', 'flag cases', 'Ã¢Â€Â¢ FAP is', 'Fraud Analytics and', 'platform with', 'case manager and', 'suite of', 'Analytics for', 'ERP systems', 'clients to', 'Accounting systems for', 'anomalies which', 'indicators of', 'fraud by', 'analytics\\n\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "freq_word = Counter(keyword)\n",
        "print(freq_word.most_common())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aURwcHXkFYe",
        "outputId": "32833b94-cb5d-4530-d7ea-97373d556183"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(Programming Languages, 1), (Machine learning, 1), (NaÃƒÂ¯ve Bayes, 1), (Random Forest, 1), (Decision Trees, 1), (Cluster Analysis, 1), (Word Embedding, 1), (Sentiment Analysis, 1), (Natural Language processing, 1), (Dimensionality reduction, 1), (Topic Modelling, 1), (Neural Nets, 1), (Database Visualizations, 1), (ElasticSearch D3.js, 1), (Angular 6, 1), (Python Flask, 1), (computer vision, 1), (CV and, 1), (understanding of, 1), (Deep learning, 1), (Education Details \n",
            "\n",
            ", 1), (Data Science Assurance Associate \n",
            "\n",
            ", 1), (Data Science Assurance Associate, 1), (Young LLP\n",
            ", 1), (Skill Details \n",
            ", 1), (JAVASCRIPT- Exprience, 1), (months\n",
            ", 1), (months\n",
            ", 1), (Python- Exprience, 1), (monthsCompany Details \n",
            ", 1), (Young LLP\n",
            ", 1), (Fraud Investigations and, 1), (Dispute Services   , 1), (Assurance\n",
            ", 1), (TECHNOLOGY ASSISTED, 1), (REVIEW\n",
            ", 1), (Technology Assisted Review, 1), (review process and, 1), (analytics and, 1), (Core member of, 1), (team helped, 1), (review platform tool from, 1), (scratch for, 1), (E discovery domain, 1), (tool implements, 1), (coding and, 1), (topic modelling by, 1), (labor costs and, 1), (time spent, 1), (lawyers review, 1), (end to, 1), (end flow of, 1), (research and, 1), (development for, 1), (classification models, 1), (analysis and, 1), (mining of, 1), (information present, 1), (text data, 1), (outputs and, 1), (precision monitoring for, 1), (TAR assists in, 1), (topic modelling from, 1), (evidence by, 1), (EY standards, 1), (models in, 1), (order to, 1), (cosine similarity, 1), (NaÃƒÂ¯ve Bayes, 1), (NMF for, 1), (topic modelling, 1), (Vader and, 1), (text blob for, 1), (sentiment analysis, 1), (Matplot lib, 1), (Tableau dashboard for, 1), (MULTIPLE DATA SCIENCE AND, 1), (USA CLIENTS, 1), (TEXT ANALYTICS, 1), (MOTOR VEHICLE CUSTOMER REVIEW DATA, 1), (customer feedback survey data for, 1), (time series analysis on, 1), (customer comments across, 1), (heat map of, 1), (terms by, 1), (survey category based, 1), (frequency of, 1), (Positive and, 1), (words across, 1), (Survey categories and, 1), (Word cloud, 1), (tableau dashboards for, 1), (reporting and, 1), (user friendly, 1), (chatbot for, 1), (Products which, 1), (questions about, 1), (hours of, 1), (reservation options and, 1), (chat bot serves, 1), (product related, 1), (overview of, 1), (tool via, 1), (QA platform and, 1), (recommendation responses so, 1), (user question to, 1), (chain of, 1), (intelligence to, 1), (pipeline of, 1), (questions as, 1), (user requirement and, 1), (language processing, 1), (topic modelling, 1), (Sentiment analysis, 1), (Word Embedding, 1), (SqlServer\n",
            "\n",
            ", 1), (INFORMATION GOVERNANCE\n",
            ", 1), (Organizations to, 1), (decisions about, 1), (information they, 1), (Information Governance portfolio synthesizes, 1), (intelligence across, 1), (data sources and, 1), (action to, 1), (organizations are, 1), (information risk, 1), (Scan data from, 1), (sources of, 1), (formats and, 1), (file formats, 1), (Meta data information, 1), (results for, 1), (search and, 1), (dashboards using, 1), (ROT Analysis on, 1), (data which, 1), (information of, 1), (data which, 1), (content that, 1), (text search analysis on, 1), (search with, 1), (methods which, 1), (security numbers, 1), (Elastic Search, 1), (Kibana\n",
            "\n",
            ", 1), (ANALYTIC PLATFORM\n",
            ", 1), (Fraud Analytics and, 1), (platform to, 1), (flag cases, 1), (Ã¢Â€Â¢ FAP is, 1), (Fraud Analytics and, 1), (platform with, 1), (case manager and, 1), (suite of, 1), (Analytics for, 1), (ERP systems, 1), (clients to, 1), (Accounting systems for, 1), (anomalies which, 1), (indicators of, 1), (fraud by, 1), (analytics\n",
            "\n",
            ", 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_freq = Counter(keyword).most_common(1)[0][1]\n",
        "for word in freq_word.keys():\n",
        "        # freq_word[word] = (freq_word[word]/max_freq)\n",
        "        freq_word[word] = (freq_word[word])\n",
        "\n",
        "freq_word.most_common(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t8anQpkkKoK",
        "outputId": "58d359b6-67ae-4063-c67a-dc60987da1ca"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Programming Languages, 1),\n",
              " (Machine learning, 1),\n",
              " (NaÃƒÂ¯ve Bayes, 1),\n",
              " (Random Forest, 1),\n",
              " (Decision Trees, 1),\n",
              " (Cluster Analysis, 1),\n",
              " (Word Embedding, 1),\n",
              " (Sentiment Analysis, 1),\n",
              " (Natural Language processing, 1),\n",
              " (Dimensionality reduction, 1),\n",
              " (Topic Modelling, 1),\n",
              " (Neural Nets, 1),\n",
              " (Database Visualizations, 1),\n",
              " (ElasticSearch D3.js, 1),\n",
              " (Angular 6, 1),\n",
              " (Python Flask, 1),\n",
              " (computer vision, 1),\n",
              " (CV and, 1),\n",
              " (understanding of, 1),\n",
              " (Deep learning, 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sent_strength={}\n",
        "for sent in cv_doc.sents:\n",
        "    for word in sent:\n",
        "        if word.text in freq_word.keys():\n",
        "            if sent in sent_strength.keys():\n",
        "                sent_strength[sent]+=freq_word[word.text]\n",
        "            else:\n",
        "                sent_strength[sent]=freq_word[word.text]\n",
        "print(sent_strength)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rz0JEFokNo6",
        "outputId": "e290ed0b-d8fe-4d9d-e0cf-f4cf7b581325"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "summarized_sentences = nlargest(3, sent_strength, key=sent_strength.get)\n",
        "print(summarized_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxgjgkqukRVq",
        "outputId": "d89287dd-6d6f-499e-ab4e-888e5ef0e0bb"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(type(summarized_sentences[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "kbBPky1ekTLn",
        "outputId": "f0b82835-6bfe-486a-c27e-4d1006574e8a"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-a5c395fc18fa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummarized_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "final_sentences = [ w.text for w in summarized_sentences ]\n",
        "summary = ' '.join(final_sentences)\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "VGgvxcHekU_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_doc1 = nlp(txt)\n",
        "for sent in cv_doc1.sents:\n",
        "  print(sent)"
      ],
      "metadata": {
        "id": "-KEj6SrfkxZR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}