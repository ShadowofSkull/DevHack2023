{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0qcuirC/k8K7OqDNxsYVm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShadowofSkull/DevHack2023/blob/main/gcolab/summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V-HSh8aXf8N",
        "outputId": "8e9bf21e-4f32-46cf-d018-be522d257b68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2023-11-02 14:31:39.825379: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-02 14:31:39.825450: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-02 14:31:39.825496: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-02 14:31:39.840182: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-02 14:31:41.091733: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2023-11-02 14:31:53.922725: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-02 14:31:53.922788: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-02 14:31:53.922829: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-02 14:31:53.932887: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-02 14:31:55.072168: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-md==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.6.0/en_core_web_md-3.6.0-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_md\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from heapq import nlargest"
      ],
      "metadata": {
        "id": "ISeWZZb3X09j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cv =\"\"\"Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃƒÂ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education Details\n",
        "\n",
        "Data Science Assurance Associate\n",
        "\n",
        "Data Science Assurance Associate - Ernst & Young LLP\n",
        "Skill Details\n",
        "JAVASCRIPT- Exprience - 24 months\n",
        "jQuery- Exprience - 24 months\n",
        "Python- Exprience - 24 monthsCompany Details\n",
        "company - Ernst & Young LLP\n",
        "description - Fraud Investigations and Dispute Services   Assurance\n",
        "TECHNOLOGY ASSISTED REVIEW\n",
        "TAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.\n",
        "* Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review.\n",
        "* Understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring for the entire tool.\n",
        "* TAR assists in predictive coding, topic modelling from the evidence by following EY standards. Developed the classifier models in order to identify \"red flags\" and fraud-related issues.\n",
        "\n",
        "Tools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃƒÂ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment analysis. Matplot lib, Tableau dashboard for reporting.\n",
        "\n",
        "MULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)\n",
        "TEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA * Received customer feedback survey data for past one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4 categories.\n",
        "* Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words across all the Survey categories and plotted Word cloud.\n",
        "* Created customized tableau dashboards for effective reporting and visualizations.\n",
        "CHATBOT * Developed a user friendly chatbot for one of our Products which handle simple questions about hours of operation, reservation options and so on.\n",
        "* This chat bot serves entire product related questions. Giving overview of tool via QA platform and also give recommendation responses so that user question to build chain of relevant answer.\n",
        "* This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant /recommended questions.\n",
        "\n",
        "Tools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer\n",
        "\n",
        "INFORMATION GOVERNANCE\n",
        "Organizations to make informed decisions about all of the information they store. The integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk.\n",
        "* Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana.\n",
        "* Preforming ROT Analysis on the data which give information of data which helps identify content that is either Redundant, Outdated, or Trivial.\n",
        "* Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally identifiable information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks.\n",
        "Tools & Technologies: Python, Flask, Elastic Search, Kibana\n",
        "\n",
        "FRAUD ANALYTIC PLATFORM\n",
        "Fraud Analytics and investigative platform to review all red flag cases.\n",
        "Ã¢Â€Â¢ FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP systems.\n",
        "* It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics\n",
        "\n",
        "Tools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js\"\"\"\n",
        "\n",
        "cv2 = \"\"\"Education Details\n",
        "January 2016 B.Sc. Information Technology Mumbai, Maharashtra University of Mumbai\n",
        "January 2012 HSC  Allahabad, Uttar Pradesh Allahabad university\n",
        "January 2010 SSC dot Net Allahabad, Uttar Pradesh Allahabad university\n",
        "Web designer and Developer Trainer\n",
        "\n",
        "Web designer and Developer\n",
        "Skill Details\n",
        "Web design- Exprience - 12 months\n",
        "Php- Exprience - 12 monthsCompany Details\n",
        "company - NetTech India\n",
        "description - Working. ( salary - 12k)\n",
        "PERSONAL INTEREST\n",
        "\n",
        "Listening to Music, Surfing net, Watching Movie, Playing Cricket.\n",
        "company - EPI Center Academy\n",
        "description - Working.  ( Salary Contract based)\n",
        "company - Aptech Charni Road\n",
        "description - Salary Contract based)\"\"\"\n",
        "job = \"\"\"University degree required.Â TEFL / TESOL / CELTA or teaching experience preferred but not necessaryPositive attitude required.Â Canada/US passport holders only\"\"\"\n",
        "job2 = \"\"\"5 years+ experience in delivering stunning front-end productsSolid understanding of lean and agile practices, in particular SCRUMSoftware engineering practices: TDD, unit/functional automated testing, CI, CD, software design and architectureOutstanding software development talent proven by great work results/experience, hobby projects or open source contributionPassion for building great products and user-interfacesOpen minded, outgoing, self-confident and positive personalityCan do attitude, great team playerFluent in englishExcellent understanding of the whole web technology stack (Ruby/Ruby on Rails, JavaScript, SQL, HTML, CSS)Knowlege about cross-plattform responsive designSkill in writing scalable and multi platform frontend code.Â Knowledge about Frontend Performance OptimizationKnowledge about frontend libraries like jQuery, YUI, prototype and also JS coding without libraries.Basic knowledge of #URL_a58bd7bd48420a1f4774598bc5f1451bdcc79baee91a357c1d69e8aede501d73#, Python etc.\"\"\""
      ],
      "metadata": {
        "id": "ROyQeSwFYEDT"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_md')\n",
        "cv.replace(\"\\n\", \"\")\n",
        "cv_doc = nlp(cv)\n",
        "print(len(list(cv_doc.sents)))\n",
        "job_doc = nlp(job)\n",
        "print(len(list(job_doc.sents)))\n",
        "# for tok in cv_doc:\n",
        "#   print(tok.text, tok.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkBeMWVVjq5j",
        "outputId": "121cd16f-a5bd-4fca-d210-6223c198d7a6"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher\n",
        "# To get nouns\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [\n",
        "    {\"POS\": \"PROPN\", \"OP\": \"+\"},\n",
        "    {\"POS\": \"NOUN\", \"OP\": \"+\"},\n",
        "\n",
        "]\n",
        "pattern1 = [\n",
        "    {\"POS\": \"NOUN\", \"OP\": \"+\"},\n",
        "    {\"POS\": \"PROPN\", \"OP\": \"+\"},\n",
        "\n",
        "]\n",
        "pattern2 = [\n",
        "    {\"POS\": \"NOUN\", \"OP\": \"+\"},\n",
        "\n",
        "]\n",
        "pattern3 = [\n",
        "    {\"POS\": \"PROPN\", \"OP\": \"+\"},\n",
        "\n",
        "]\n",
        "matcher.add(\"label\", [pattern, pattern1, pattern2, pattern3], greedy=\"LONGEST\")\n",
        "matches = matcher(cv_doc)\n",
        "# To sort in the order of occurance in the resume\n",
        "matches.sort(key = lambda x: x[1])\n",
        "print(len(matches))\n",
        "keyword = []\n",
        "stopwords = list(STOP_WORDS)\n",
        "\n",
        "\n",
        "for mat in matches:\n",
        "  text = cv_doc[mat[1]: mat[2]]\n",
        "  if not (text.text in stopwords or text.text in punctuation):\n",
        "    keyword.append(text.text)\n",
        "    # print(mat, text.text)\n",
        "\n",
        "print(keyword)\n",
        "\n",
        "# pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
        "# pos_tag = ['PROPN', 'NOUN']\n",
        "\n",
        "# for token in cv_doc:\n",
        "#     if(token.text in stopwords or token.text in punctuation):\n",
        "#         continue\n",
        "    # if(token.pos_ in pos_tag):\n",
        "    #     keyword.append(token.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEGEgYGBkCrh",
        "outputId": "c5154448-701d-4ec0-f376-6ce77a578a3c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "266\n",
            "['Skills', 'Programming Languages', 'Python', 'pandas', 'scipy', 'scikit', 'learn', 'matplotlib', 'Sql', 'Java', 'JavaScript', 'JQuery', 'Machine learning', 'Regression', 'SVM', 'NaÃƒÂ¯ve Bayes', 'KNN', 'Random Forest', 'Decision Trees', 'techniques', 'Cluster Analysis', 'Word Embedding', 'Sentiment Analysis', 'Natural Language processing', 'Dimensionality reduction', 'Topic Modelling', 'LDA', 'NMF', 'PCA', 'Neural Nets', 'Database Visualizations', 'Mysql', 'SqlServer', 'Cassandra', 'Hbase', 'ElasticSearch D3.js', 'kibana', 'matplotlib', 'ggplot', 'Tableau', 'Others', 'Expression', 'HTML', 'CSS', 'Angular', 'Logstash', 'Kafka', 'Python Flask', 'Git', 'Docker', 'computer vision', 'CV', 'understanding', 'Deep learning', 'Education Details', 'Data Science Assurance Associate', 'Data Science Assurance Associate', 'Ernst', 'Young LLP', 'Skill Details', 'JAVASCRIPT- Exprience', 'months', 'Exprience', 'months', 'Python- Exprience', 'monthsCompany Details', 'company', 'Ernst', 'Young LLP', 'description', 'Fraud Investigations', 'Dispute Services', 'Assurance', 'TECHNOLOGY', 'REVIEW', 'TAR', 'Technology Assisted Review', 'review process', 'analytics', 'reports', 'Core member', 'team', 'review platform tool', 'scratch', 'E discovery domain', 'tool', 'coding', 'topic modelling', 'reviews', 'labor costs', 'time', 'lawyers', 'end', 'end flow', 'solution', 'research', 'development', 'classification models', 'analysis', 'mining', 'information', 'text data', 'outputs', 'precision monitoring', 'tool', 'TAR assists', 'coding', 'topic modelling', 'evidence', 'EY standards', 'models', 'order', 'flags', 'fraud', 'issues', 'Tools', 'Technologies', 'Python', 'scikit', 'learn', 'tfidf', 'word2vec', 'doc2vec', 'cosine similarity', 'NaÃƒÂ¯ve Bayes', 'LDA', 'NMF', 'topic modelling', 'Vader', 'text blob', 'sentiment analysis', 'Matplot lib', 'Tableau dashboard', 'MULTIPLE DATA SCIENCE', 'PROJECTS', 'USA CLIENTS', 'TEXT ANALYTICS', 'MOTOR VEHICLE CUSTOMER REVIEW DATA', 'customer feedback survey data', 'year', 'sentiment', 'Positive', 'Negative', 'Neutral', 'time series analysis', 'customer comments', 'categories', 'heat map', 'terms', 'survey category', 'frequency', 'words', 'Positive', 'words', 'Survey categories', 'Word cloud', 'tableau dashboards', 'reporting', 'visualizations', 'CHATBOT', 'user', 'chatbot', 'Products', 'questions', 'hours', 'operation', 'reservation options', 'chat bot', 'product', 'questions', 'overview', 'tool', 'QA platform', 'recommendation responses', 'user question', 'chain', 'answer', 'intelligence', 'pipeline', 'questions', 'user requirement', 'questions', 'Tools', 'Technologies', 'Python', 'language processing', 'NLTK', 'spacy', 'topic modelling', 'Sentiment analysis', 'Word Embedding', 'scikit', 'learn', 'JavaScript', 'JQuery', 'SqlServer', 'INFORMATION GOVERNANCE', 'Organizations', 'decisions', 'information', 'Information Governance portfolio', 'intelligence', 'data sources', 'action', 'organizations', 'information risk', 'Scan data', 'sources', 'formats', 'file formats', 'Meta data information', 'results', 'search', 'dashboards', 'kibana', 'ROT Analysis', 'data', 'information', 'data', 'content', 'Outdated', 'Trivial', 'text search analysis', 'search', 'methods', 'PII', 'information', 'security numbers', 'addresses', 'names', 'cyber', 'attacks', 'Tools', 'Technologies', 'Python', 'Flask', 'Elastic Search', 'Kibana', 'ANALYTIC PLATFORM', 'Fraud Analytics', 'platform', 'flag cases', 'Ã¢Â€Â¢ FAP', 'Fraud Analytics', 'platform', 'case manager', 'suite', 'Analytics', 'ERP systems', 'clients', 'Accounting systems', 'anomalies', 'indicators', 'fraud', 'analytics', 'Tools', 'Technologies', 'HTML', 'JavaScript', 'SqlServer', 'JQuery', 'CSS', 'Bootstrap', 'Node.js', 'D3.js', 'DC.js']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "freq_word = Counter(keyword)\n",
        "print(freq_word.most_common())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aURwcHXkFYe",
        "outputId": "230ae23f-365a-4389-ffc9-205760f0ba63"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Python', 4), ('topic modelling', 4), ('information', 4), ('Tools', 4), ('Technologies', 4), ('questions', 4), ('scikit', 3), ('learn', 3), ('JavaScript', 3), ('JQuery', 3), ('SqlServer', 3), ('tool', 3), ('matplotlib', 2), ('NaÃƒÂ¯ve Bayes', 2), ('Word Embedding', 2), ('LDA', 2), ('NMF', 2), ('kibana', 2), ('HTML', 2), ('CSS', 2), ('Data Science Assurance Associate', 2), ('Ernst', 2), ('Young LLP', 2), ('months', 2), ('analytics', 2), ('coding', 2), ('fraud', 2), ('Positive', 2), ('words', 2), ('intelligence', 2), ('search', 2), ('data', 2), ('Fraud Analytics', 2), ('platform', 2), ('Skills', 1), ('Programming Languages', 1), ('pandas', 1), ('scipy', 1), ('Sql', 1), ('Java', 1), ('Machine learning', 1), ('Regression', 1), ('SVM', 1), ('KNN', 1), ('Random Forest', 1), ('Decision Trees', 1), ('techniques', 1), ('Cluster Analysis', 1), ('Sentiment Analysis', 1), ('Natural Language processing', 1), ('Dimensionality reduction', 1), ('Topic Modelling', 1), ('PCA', 1), ('Neural Nets', 1), ('Database Visualizations', 1), ('Mysql', 1), ('Cassandra', 1), ('Hbase', 1), ('ElasticSearch D3.js', 1), ('ggplot', 1), ('Tableau', 1), ('Others', 1), ('Expression', 1), ('Angular', 1), ('Logstash', 1), ('Kafka', 1), ('Python Flask', 1), ('Git', 1), ('Docker', 1), ('computer vision', 1), ('CV', 1), ('understanding', 1), ('Deep learning', 1), ('Education Details', 1), ('Skill Details', 1), ('JAVASCRIPT- Exprience', 1), ('Exprience', 1), ('Python- Exprience', 1), ('monthsCompany Details', 1), ('company', 1), ('description', 1), ('Fraud Investigations', 1), ('Dispute Services', 1), ('Assurance', 1), ('TECHNOLOGY', 1), ('REVIEW', 1), ('TAR', 1), ('Technology Assisted Review', 1), ('review process', 1), ('reports', 1), ('Core member', 1), ('team', 1), ('review platform tool', 1), ('scratch', 1), ('E discovery domain', 1), ('reviews', 1), ('labor costs', 1), ('time', 1), ('lawyers', 1), ('end', 1), ('end flow', 1), ('solution', 1), ('research', 1), ('development', 1), ('classification models', 1), ('analysis', 1), ('mining', 1), ('text data', 1), ('outputs', 1), ('precision monitoring', 1), ('TAR assists', 1), ('evidence', 1), ('EY standards', 1), ('models', 1), ('order', 1), ('flags', 1), ('issues', 1), ('tfidf', 1), ('word2vec', 1), ('doc2vec', 1), ('cosine similarity', 1), ('Vader', 1), ('text blob', 1), ('sentiment analysis', 1), ('Matplot lib', 1), ('Tableau dashboard', 1), ('MULTIPLE DATA SCIENCE', 1), ('PROJECTS', 1), ('USA CLIENTS', 1), ('TEXT ANALYTICS', 1), ('MOTOR VEHICLE CUSTOMER REVIEW DATA', 1), ('customer feedback survey data', 1), ('year', 1), ('sentiment', 1), ('Negative', 1), ('Neutral', 1), ('time series analysis', 1), ('customer comments', 1), ('categories', 1), ('heat map', 1), ('terms', 1), ('survey category', 1), ('frequency', 1), ('Survey categories', 1), ('Word cloud', 1), ('tableau dashboards', 1), ('reporting', 1), ('visualizations', 1), ('CHATBOT', 1), ('user', 1), ('chatbot', 1), ('Products', 1), ('hours', 1), ('operation', 1), ('reservation options', 1), ('chat bot', 1), ('product', 1), ('overview', 1), ('QA platform', 1), ('recommendation responses', 1), ('user question', 1), ('chain', 1), ('answer', 1), ('pipeline', 1), ('user requirement', 1), ('language processing', 1), ('NLTK', 1), ('spacy', 1), ('Sentiment analysis', 1), ('INFORMATION GOVERNANCE', 1), ('Organizations', 1), ('decisions', 1), ('Information Governance portfolio', 1), ('data sources', 1), ('action', 1), ('organizations', 1), ('information risk', 1), ('Scan data', 1), ('sources', 1), ('formats', 1), ('file formats', 1), ('Meta data information', 1), ('results', 1), ('dashboards', 1), ('ROT Analysis', 1), ('content', 1), ('Outdated', 1), ('Trivial', 1), ('text search analysis', 1), ('methods', 1), ('PII', 1), ('security numbers', 1), ('addresses', 1), ('names', 1), ('cyber', 1), ('attacks', 1), ('Flask', 1), ('Elastic Search', 1), ('Kibana', 1), ('ANALYTIC PLATFORM', 1), ('flag cases', 1), ('Ã¢Â€Â¢ FAP', 1), ('case manager', 1), ('suite', 1), ('Analytics', 1), ('ERP systems', 1), ('clients', 1), ('Accounting systems', 1), ('anomalies', 1), ('indicators', 1), ('Bootstrap', 1), ('Node.js', 1), ('D3.js', 1), ('DC.js', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_freq = Counter(keyword).most_common(1)[0][1]\n",
        "for word in freq_word.keys():\n",
        "      freq_word[word] = (freq_word[word]/max_freq)\n",
        "\n",
        "freq_word.most_common(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t8anQpkkKoK",
        "outputId": "b59bf656-823c-4cf8-bb59-ce5f4409a361"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Python', 1.0),\n",
              " ('topic modelling', 1.0),\n",
              " ('information', 1.0),\n",
              " ('Tools', 1.0),\n",
              " ('Technologies', 1.0),\n",
              " ('questions', 1.0),\n",
              " ('scikit', 0.75),\n",
              " ('learn', 0.75),\n",
              " ('JavaScript', 0.75),\n",
              " ('JQuery', 0.75),\n",
              " ('SqlServer', 0.75),\n",
              " ('tool', 0.75),\n",
              " ('matplotlib', 0.5),\n",
              " ('NaÃƒÂ¯ve Bayes', 0.5),\n",
              " ('Word Embedding', 0.5),\n",
              " ('LDA', 0.5),\n",
              " ('NMF', 0.5),\n",
              " ('kibana', 0.5),\n",
              " ('HTML', 0.5),\n",
              " ('CSS', 0.5)]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sent_strength={}\n",
        "for sent in cv_doc.sents:\n",
        "    for word in sent:\n",
        "        if word.text in freq_word.keys():\n",
        "            if sent in sent_strength.keys():\n",
        "                sent_strength[sent]+=freq_word[word.text]\n",
        "            else:\n",
        "                sent_strength[sent]=freq_word[word.text]\n",
        "print(sent_strength)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rz0JEFokNo6",
        "outputId": "17678c52-7387-47a2-944b-1eae41200d43"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery.: 5.75, Machine learning: Regression, SVM, NaÃƒÂ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets.: 2.25, Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau.: 3.5, Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.: 4.5, Education Details\n",
            "\n",
            "Data Science Assurance Associate\n",
            "\n",
            "Data Science Assurance Associate - Ernst & Young LLP\n",
            ": 1.0, Exprience - 24 months\n",
            "jQuery- Exprience - 24 months\n",
            "Python-: 1.5, Exprience - 24 monthsCompany: 0.25, Details\n",
            "company - Ernst & Young LLP\n",
            "description - Fraud Investigations and Dispute Services   Assurance\n",
            "TECHNOLOGY ASSISTED REVIEW\n",
            "TAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.\n",
            ": 2.75, Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review.\n",
            "*: 3.75, Understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data.: 3.5, Worked on analyzing the outputs and precision monitoring for the entire tool.\n",
            ": 1.0, TAR assists in predictive coding, topic modelling from the evidence by following EY standards.: 1.0, Developed the classifier models in order to identify \"red flags\" and fraud-related issues.\n",
            "\n",
            "Tools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃƒÂ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment analysis.: 8.5, Matplot lib, Tableau dashboard for reporting.\n",
            "\n",
            ": 0.5, MULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)\n",
            ": 0.25, TEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA *: 0.25, Received customer feedback survey data for past one year.: 0.75, Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4 categories.\n",
            ": 2.0, * Created heat map of terms by survey category based on frequency of words *: 1.0, Extracted Positive and Negative words across all the Survey categories and plotted Word cloud.\n",
            ": 1.5, Created customized tableau dashboards for effective reporting and visualizations.\n",
            "CHATBOT *: 1.0, Developed a user friendly chatbot for one of our Products which handle simple questions about hours of operation, reservation options and so on.\n",
            "*: 2.25, This chat bot serves entire product related questions.: 1.25, Giving overview of tool via QA platform and also give recommendation responses so that user question to build chain of relevant answer.\n",
            ": 2.25, This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant /recommended questions.\n",
            "\n",
            ": 3.0, Tools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer\n",
            "\n",
            "INFORMATION GOVERNANCE\n",
            "Organizations to make informed decisions about all of the information they store.: 9.0, The integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk.\n",
            ": 2.75, Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana.\n",
            ": 4.25, Preforming ROT Analysis on the data which give information of data which helps identify content that is either Redundant, Outdated, or Trivial.\n",
            ": 2.75, Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally identifiable information (social security numbers, addresses, names, etc.): 3.25, which frequently targeted during cyber-attacks.\n",
            "Tools & Technologies: Python, Flask, Elastic Search, Kibana\n",
            "\n",
            "FRAUD ANALYTIC PLATFORM\n",
            "Fraud Analytics and investigative platform to review all red flag cases.\n",
            ": 4.75, Ã¢Â€Â¢ FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP systems.\n",
            ": 1.25, It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics\n",
            "\n",
            "Tools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js: 8.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "summarized_sentences = nlargest(10, sent_strength, key=sent_strength.get)\n",
        "print(summarized_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxgjgkqukRVq",
        "outputId": "eacefc16-051e-481c-84a6-7e76cbe0ad4c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Tools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer\n",
            "\n",
            "INFORMATION GOVERNANCE\n",
            "Organizations to make informed decisions about all of the information they store., Developed the classifier models in order to identify \"red flags\" and fraud-related issues.\n",
            "\n",
            "Tools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃƒÂ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment analysis., It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics\n",
            "\n",
            "Tools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js, Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery., which frequently targeted during cyber-attacks.\n",
            "Tools & Technologies: Python, Flask, Elastic Search, Kibana\n",
            "\n",
            "FRAUD ANALYTIC PLATFORM\n",
            "Fraud Analytics and investigative platform to review all red flag cases.\n",
            ", Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning., Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana.\n",
            ", Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review.\n",
            "*, Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau., Understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "final_sentences = [ w.text for w in summarized_sentences ]\n",
        "summary = ' '.join(final_sentences)\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "VGgvxcHekU_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b9a62c8-c3f6-49f6-e232-250102e071d1"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer\n",
            "\n",
            "INFORMATION GOVERNANCE\n",
            "Organizations to make informed decisions about all of the information they store. Developed the classifier models in order to identify \"red flags\" and fraud-related issues.\n",
            "\n",
            "Tools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃƒÂ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment analysis. It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics\n",
            "\n",
            "Tools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. which frequently targeted during cyber-attacks.\n",
            "Tools & Technologies: Python, Flask, Elastic Search, Kibana\n",
            "\n",
            "FRAUD ANALYTIC PLATFORM\n",
            "Fraud Analytics and investigative platform to review all red flag cases.\n",
            " Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning. Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana.\n",
            " Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review.\n",
            "* Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. Understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "job_doc1 = nlp(\"\"\"\"\"\")\n",
        "for sent in job_doc1.sents:\n",
        "  print(sent)\n",
        "sum = nlp(summary)\n",
        "# cv_doc1 = nlp(cv)\n",
        "# for sent in cv_doc1.sents:\n",
        "#   print(sent)\n",
        "job_doc1.similarity(sum)"
      ],
      "metadata": {
        "id": "-KEj6SrfkxZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26fcfd6b-872d-4824-8dd6-61ff9fc8c8b5"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passionate aboutÂ chocolate and cocoa productsEngaging, confident, and inspirational speakerSelf-motivated, and service orientedExperienced inÂ using social media, including facebook and twitterAble to innovate and respond flexibly to audience needsFluent in EnglishEffective team member and leaderThis role is predominately London based with some regional travel.\n",
            "Â  \n",
            "The hours of work are weekdays, evenings and weekends, with up to 3 evenings per week and every weekend.\n",
            "Min: 20 hours per weekYou will need to be flexible and cover ad-hoc events within 3 daysâ€™ notice\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.890160205707202"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    }
  ]
}